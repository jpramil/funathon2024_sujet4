{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4dd2dc2-30c8-421f-acc2-baaafe58a389",
   "metadata": {},
   "source": [
    "# Funathon 2024 - Sujet 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a28737-866c-4f4d-836d-9f825f426aad",
   "metadata": {},
   "source": [
    "Responsables :\n",
    "- Olivier Meslin, SSP Lab\n",
    "- Antoine Palazzolo, SSP Lab\n",
    "- Romain Avouac, DIIT\n",
    "\n",
    "Attention, si vous avez déjà fait le [sujet 5](https://github.com/InseeFrLab/funathon2023_sujet5) du funathon 2023 _\"Analyse textuelle des commentaires clients de services de commande de repas en ligne\"_, vous n'apprendrez rien en faisant celui-ci !\n",
    "\n",
    "<br>\n",
    "\n",
    "Dans l'ensemble le sujet est considéré comme de difficulté intermédiaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d8dc9-1481-489b-91ed-bb611a067a3d",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Niveau technique</th>\n",
    "      <th>Enjeux</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><span class=\"level-button level-debutant\">Débutant</span></td>\n",
    "      <td>Lire le sujet et les ressources associées, reprendre et comprendre les corrections.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><span class=\"level-button level-intermediaire\">Intermédiaire</span></td>\n",
    "      <td>Chercher à faire les exercices et ne regarder les aides que si nécessaire.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><span class=\"level-button level-expert\">Expert</span></td>\n",
    "      <td>Passer les questions intermédiaires. Compléter les données en scrapant (toujours éthiquement) d'autres sources. Maximiser les performances des modèles de prédiction des notes.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a78f28c-cd24-4564-a18f-b1a425e0b779",
   "metadata": {},
   "source": [
    "# Analyse textuelle des commentaires clients de sites de vente de billets d'avion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9bb9cf-32c4-4d10-8d22-f4977f139b2c",
   "metadata": {},
   "source": [
    "## Avant de commencer..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f33b310-b100-4f10-9355-d1239fc5c162",
   "metadata": {},
   "source": [
    "Ce sujet, disponible uniquement en Python, porte sur deux thématiques principales :\n",
    "- Le web scraping\n",
    "- Le NLP\n",
    "\n",
    "Les deux parties sont indépendantes l'une de l'autre, il est donc possible de n'en faire qu'une des deux.\n",
    "\n",
    "Si jamais vous n'êtes pas familiers avec l'un de ces sujets (ou les deux), nous ne saurions que trop vous recommander de jeter un oeil aux ressources suivantes :\n",
    "- Débuter en web scraping : https://pythonds.linogaliana.fr/webscraping/\n",
    "- Web scraping et bonnes pratiques : https://github.com/InseeFrLab/formation-webscraping\n",
    "- Débuter en NLP : https://pythonds.linogaliana.fr/course/nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feff71e0-37e9-4d64-86ee-d56aad2179f6",
   "metadata": {},
   "source": [
    "Exécutez la cellule ci-dessous pour installer les packages nécessaires au sujet :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d56c1a6-fdb7-404a-81fd-503d5f4a0d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b22eda3-8c0f-4c06-b89b-ad9ed8fc3a6b",
   "metadata": {},
   "source": [
    "## Partie 1 : Scraping d'avis sur Trustpilot\n",
    "\n",
    "Premier point de contact : Antoine Palazzolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59941e3-e3e0-4aee-bc84-a0ea07318d5c",
   "metadata": {},
   "source": [
    "Pour pouvoir faire de l'analyse textuelle de commentaires clients, la première chose dont nous avons besoin c'est justement d'une base d'avis et de commentaires.\n",
    "Vous pourrez trouver de tels avis sur à peu près n'importe quel site de vente en ligne ou bien sur un certain nombre de comparateurs.\n",
    "En revanche, comment récupérer l'information depuis une page Internet pour nous constituer un jeu de données sur lequel travailler ?\n",
    "\n",
    "Eh bien c'est justement là qu'intervient le _web scraping_, qui permet de collecter automatiquement de l'information d'un site web, que ce soit du texte, des images, des tableaux, sans avoir à parcourir toutes les pages soi-même en faisant un copier-coller à la main du contenu.\n",
    "Le _web scraping_ est donc un outil très puissant, mais à utiliser avec des pincettes.\n",
    "Cela doit plutôt être vu comme un dernier recours, lorsqu'il n'est pas possible d'accéder aux données plus facilement.\n",
    "\n",
    "En effet, de nombreuses contraintes juridiques encadrent par exemple le _web scraping_, il n'est pas possible de faire ce que l'on veut.\n",
    "Qui plus est, de plus en plus de sites apprennent à se défendre contre cette collecte automatique de leurs données, rendant la tâche plus difficile.\n",
    "Pour en savoir plus sur ces thématiques, n'hésitez pas à consulter cette formation : https://inseefrlab.github.io/formation-webscraping/.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7881c38-f813-4edb-81af-60867441cb1a",
   "metadata": {},
   "source": [
    "Les données mises à disposition pour ce sujet ont été extraites du site https://fr.trustpilot.com/.\n",
    "Votre première tâche va être de créer votre propre scraper pour pouvoir recréer une base similaire.\n",
    "\n",
    "Afin de ne pas surcharger le traffic du site, nous n'allons pas vous demander d'en scraper l'intégralité.\n",
    "Vous allez donc vous limiter à une entreprise de votre choix, par exemple parmi celles de la catégorie Compagnies aériennes & aéroports : https://fr.trustpilot.com/categories/airlines_air_travel.\n",
    "Cliquez sur le lien et promenez-vous sur le site.\n",
    "\n",
    "Vous pouvez également aller chercher manuellement les compagnies aériennes que vous préférez étudier (ex: Air France, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f4a85-4502-4dab-8cc6-2972a3e3dff2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Bien préparer son scraping : découvrir le site ciblé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5665ee-b086-48b4-a1da-19c263062ced",
   "metadata": {},
   "source": [
    "La première chose à faire pour tout bon adepte de scraping, c'est apprendre à connaître la page web cible :\n",
    "- A quoi ressemble l'url du site ? Y a-t-il un radical commun lorsque l'on passe d'une page à une autre ? Par exemple ici lorsque l'on change d'entreprise comparée ou que l'on va chercher les avis les plus reculés ?\n",
    "- Quelles sont les informations disponibles sur la page ? Y a-t-il besoin de cliquer sur un bouton pour les faire apparaître ?\n",
    "- Les différentes pages que je souhaite scraper ont-elles bien des formats similaires ? La construction des pages change-t-elle d'une entreprise comparée à une autre ?\n",
    "- Les commentaires sont-ils tous bien en français ?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b655c8-4d2e-4bdd-8585-a9ff36922dc9",
   "metadata": {},
   "source": [
    "Il faut ensuite aller un peu plus loin dans l'analyse et regarder la structure HTML de la page ciblée.\n",
    "Si vous n'êtes pas familiers avec cette partie, n'hésitez pas à consulter les ressources précédemment citées.\n",
    "\n",
    "Tout d'abord, choisissez l'entreprise dont vous allez extraire les commentaires et cliquez sur sa page Trustpilot.\n",
    "A présent, après un clic droit sur un élément de la page, il suffit de cliquer sur \"Inspecter l'élément\" pour naviguer dans l'architecture de la page et analyser le positionnement de l'élément sélectionné.\n",
    "Cliquez sur divers éléments de la page pour vous familiariser avec sa structure.\n",
    "- Comment sont rangés les commentaires ? Comprenez-vous bien l'arborescence de la page ?\n",
    "- Regardez les balises autour de chaque note, y a-t-il un pattern ou un élément commun qui permet de les identifier et les distinguer des autres ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a6eb8-7356-434c-ac49-23c598f97b73",
   "metadata": {},
   "source": [
    "### 2. Premiers pas : récupérer l'information du nombre de pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db647d1c-6a1b-4c35-ab23-76ee6c837808",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da2e6b-b6cf-46f5-80e3-a635f7d43bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "radical_trustpilot = 'https://fr.trustpilot.com/review/' # TODO\n",
    "company = 'www.airfrance.fr' # TODO\n",
    "\n",
    "url_company = radical_trustpilot + company\n",
    "print(url_company)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15cfc33-ce00-4508-b4c6-23478a4c5725",
   "metadata": {},
   "source": [
    "Maintenant que vous avez votre premier lien à scraper, il va falloir envoyer une __requête__ au site pour demander à en récupérer le contenu, sous le format HTML.\n",
    "C'est sur le contenu retourné que nous travaillerons ensuite.\n",
    "\n",
    "En Python, un package permet facilement de faire ces requêtes, il s'agit de _requests_, et plus précisément de la fonction ```requests.get()```, qui prend en argument l'url ciblé.\n",
    "En plus du lien à requêter, la fonction peut prendre d'autres arguments, appelés __headers__, comme _User_Agent_ ou _From_ qui permettent de s'identifier lors de la requête envoyée au site.\n",
    "\n",
    "Pourquoi s'identifier me demanderez-vous ? Eh bien tout d'abord parce qu'il s'agit là d'une bonne pratique de scraping.\n",
    "Les sites reçoivent parfois énormément de requêtes sur leurs pages, par exemple à cause de scrapers trop gourmands, et cela peut leur créer des problèmes.\n",
    "Ils peuvent donc être amenés à par exemple bloquer les adresses IP des utilisateurs jugés abusifs.\n",
    "S'identifier clairement dans ses requêtes, en explicitant le but de la collecte, permet de faire preuve de transparence.\n",
    "Même si en pratique cela n'arrive que très peu, cela laisse alors la possibilité au propriétaire du site de contacter le scraper afin d'avoir plus d'informations sur le but de la collecte, et possiblement offrir l'accès aux données sans scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09af359-c980-4b23-a51e-24c8b8b3eaa6",
   "metadata": {},
   "source": [
    "Une autre bonne pratique pour ne pas être trop agressif vis-à-vis du site scrapé est d'étaler ses requêtes dans le temps, par exemple en forçant votre code à prendre une pause d'au moins 3 secondes entre chaque requête effectuée si vous visitez plusieurs pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e42e5c-6fbd-469d-9524-120e143df29a",
   "metadata": {},
   "source": [
    "Ici, complétez les headers ci-dessous avec vos informations pour faire preuve de transparence auprès de Trustpilot :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7625b4e-3d17-4298-af07-1293dd8658a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Julien, stagiaire en datascience chez Insee, collecte à but pédagogique', # Nom, entité, but de la collecte, informations pertinentes\n",
    "    'From': 'julien.pramil@insee.fr' # Coordonnées à laisser au propriétaire du site en cas de besoin\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9cab2-4d9a-4594-b2be-a6cd554c5409",
   "metadata": {},
   "source": [
    "Nous réutiliserons ces headers dans toutes les requêtes du sujet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f91af-4057-48ad-9692-375c46ae2302",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Créer sa première requête"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5778b51a-1052-4611-b375-1d208bdc8cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708dde0f-286f-474d-9b37-58570f38ac17",
   "metadata": {},
   "source": [
    "Utilisez la fonction ```requests.get()``` et vos headers pour requêter votre site, puis utilisez la méthode ```.text``` pour récupérer le contenu HTML de la page désirée sous forme de texte.\n",
    "\n",
    "Vous pouvez aussi déjà prendre le réflexe d'ajouter un ```time.sleep(3)``` à l'issue de chaque requête effectuée pour ne pas surcharger le site lorsque nous aurons davantage de requêtes à faire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d937339-d024-4b88-8280-f000bfa4e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_text = requests.get(url_company, headers=headers).text\n",
    "print(request_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09374891-4340-4d36-b5da-d8b3681a69b0",
   "metadata": {},
   "source": [
    "Si vous avez bien fait les choses, vous devriez maintenant avoir quelque chose d'assez illisible, c'est normal.\n",
    "Il va maintenant falloir faire appel à un autre package pour __parser__ cette chaîne de caractères en une arborescence plus exploitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1e5d4-157b-49dd-8d18-725b8d9b2ab6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Parsing d'un document HTML : BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c9a1f-9159-4e2d-9d17-6339c84f9301",
   "metadata": {},
   "source": [
    "La fonction ```BeautifulSoup()```, du package du même nom, est ce qui va nous permettre de faire ce parsing.\n",
    "Rien de tel que l'essayer sur notre texte pour voir quel est son effet :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68869c4-353b-4260-9ec5-df3f68baf6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a2f35f-97aa-4193-bda4-80878e14c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(request_text,'html.parser')\n",
    "\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819f7ba7-0611-4f16-a9ad-e9e9b91eddc2",
   "metadata": {},
   "source": [
    "Normalement ça a déjà une meilleure tête !\n",
    "On dira que le contenu HTML est désormais sous la forme d'une _soupe_.\n",
    "L'idée maintenant va être de naviguer parmi l'arborescence des balises dans cette _soupe_ pour aller chercher l'information que l'on souhaite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d7e70-8f4e-4e0d-b7fb-e5c478da1420",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Chercher un élément dans l'arborescence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00a3979-1674-4749-988c-32456bc2e933",
   "metadata": {},
   "source": [
    "Deux méthodes sont particulièrement utiles lorsque l'on travaille avec BeautifulSoup :\n",
    "\n",
    "- ```soup.find(type_de_balise, {'class': classe_de_la_balise)```, pour trouver le premier le premier élément correspondant à la recherche effectuée\n",
    "    + La méthode ```.text``` permet ensuite d'en extraire le contenu textuel affiché sur le site\n",
    "- ```soup.find_all(type_de_balise, {'class': classe_de_la_balise)```, pour renvoyer la liste de tous les éléments correspondant à la recherche effectuée\n",
    "\n",
    "Les types de balise sont souvent assez standards : ```div```, ```a```, ```span```.\n",
    "Quant aux noms de classes, ils ne sont pas toujours très explicites, mais que cela ne vous décourage pas !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed5398-c1ad-434d-87cc-cf59d248203d",
   "metadata": {},
   "source": [
    "Pour commencer, entraînez-vous à manipuler ces deux fonctions en récupérant des informations diverses sur le site. Vous pouvez également imbriquer plusieurs de ces fonctions les unes après les autres, l'output d'une recherche pouvant être une plus petite _soupe_ pour donnée en input d'une nouvelle recherche.\n",
    "\n",
    "Quand vous êtes prêts, utilisez les méthodes ci-dessus pour associer à la variable ```nb_pages``` le nombre de pages d'avis pour l'entreprise considérée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ebc6c-b8d5-42a1-b4a8-4617ae863231",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_classe = \"typography_body-l__KUYFJ typography_appearance-subtle__8_H2l styles_text__W4hWi\"\n",
    "boutons_pages = soup.find(\n",
    "            'span', {'class':ma_classe}\n",
    "        ).text\n",
    "print(boutons_pages)\n",
    "print(type(boutons_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e76c5bd-fcc3-4c13-8bc8-6d99510da818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_pages_reviews(url_company):\n",
    "\n",
    "    time.sleep(3)\n",
    "    request_text = requests.get(url_company, headers=headers).text\n",
    "    soup = BeautifulSoup(request_text, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        boutons_pages = soup.find(\n",
    "            'div', {'class':'styles_pagination__6VmQv'}\n",
    "        ).find_all(\n",
    "            'span', {'class':'typography_heading-xxs__QKBS8 typography_appearance-inherit__D7XqR typography_disableResponsiveSizing__OuNP7'}\n",
    "        )\n",
    "        last_page = int(boutons_pages[-2].text)  # Dernier bouton = \"Page Suivante\"\n",
    "        return last_page\n",
    "\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "get_nb_pages_reviews(url_company)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb2b14-b3d2-48d6-9b12-255959b5d5f3",
   "metadata": {},
   "source": [
    "Indice : Le nombre de pages est accessible au bas de la page. En revanche, la balise contenant l'information et son nom de classe ne sont peut-être pas uniques..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c84cd-c0fb-4958-ab63-bd4f68540e85",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Mise sous fonction (et corrigé)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c8006-a15d-4d45-8d48-7c16cce4a4ac",
   "metadata": {},
   "source": [
    "Maintenant que vous avez compris le principe, il est temps de regrouper tout ce qui a été fait en une fonction ```get_nb_pages_review()``` prenant en entrée un cible et renvoyant le nombre de pages d'avis.\n",
    "Attention cette fois à rajouter une condition d'exception si jamais aucune balise du type désirée n'est trouvée..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052a5ea-379a-45f9-bd7d-d1fb07e2dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_pages_reviews(url_company):\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83c6f8-8886-4b6b-a911-d9d365450109",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_test_1 = radical_trustpilot + \"deligreens.com\"\n",
    "get_nb_pages_reviews(url_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eab3de-3965-4f4d-be8d-2e24a9d1cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_test_2 = radical_trustpilot + \"entreprise_mystere.com\"  # Entreprise qui n'existe pas\n",
    "get_nb_pages_reviews(url_test_2)  # Doit pouvoir s'exécuter sans erreur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde2543-0c39-426b-9513-403f4917a5b1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Dérouler pour révéler un exemple de corrigé</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def get_nb_pages_reviews(url_company):\n",
    "\n",
    "    time.sleep(3)\n",
    "    request_text = requests.get(url_company, headers=headers).text\n",
    "    soup = BeautifulSoup(request_text, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        boutons_pages = soup.find(\n",
    "            'div', {'class':'styles_pagination__6VmQv'}\n",
    "        ).find_all(\n",
    "            'span', {'class':'typography_heading-xxs__QKBS8 typography_appearance-inherit__D7XqR typography_disableResponsiveSizing__OuNP7'}\n",
    "        )\n",
    "        last_page = int(boutons_pages[-2].text)  # Dernier bouton = \"Page Suivante\"\n",
    "        return last_page\n",
    "\n",
    "    except:\n",
    "        return 0\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c664b-2380-4bb0-82fe-809560c42b64",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Autre corrigé trouvé par un participant en 2023, plus simple :</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def get_nb_pages_reviews(url_company):\n",
    "\n",
    "    time.sleep(3)\n",
    "    request_text = requests.get(url_company, headers=headers).text\n",
    "    soup = BeautifulSoup(request_text, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        bouton_derniere_page = soup.find('a', {'name':'pagination-button-last'}).text\n",
    "        last_page = int(bouton_derniere_page)\n",
    "        return last_page\n",
    "\n",
    "    except:\n",
    "        return 0\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Il existe effectivement d'autres arguments aux balises que la ```class```, comme par exemple ici le ```name```.\n",
    "Ces autres arguments peuvent parfois être uniques et donc plus pratiques à utiliser, comme c'est le cas ici.\n",
    "On s'économise l'usage d'un ```find_all()``` et c'est bien plus lisible !\n",
    "Il faut simplement fouiller un peu plus dans les balises HTML pour les repérer :)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510209e-e411-4d9b-a8dd-00d707d2501d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Mise en pratique : récupérer les avis d'une entreprise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9878dc47-9f82-44ae-8bbd-8e578c3a53cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Récupérer les informations au sein d'une review donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa22f3b4-3364-4d3a-bbe5-e00e1705a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24236cd6-1690-4ac3-804b-be366356e7a6",
   "metadata": {},
   "source": [
    "Imaginez que vous avez sous la main le code HTML (sous forme de _soupe_) relatif à une seule review sur votre page (vous pouvez voir en inspectant la page que chaque review est contenue dans une balise ```div``` distincte, les unes après les autres).\n",
    "\n",
    "Nous allons maintenant tâcher de récupérer les informations relatives à une review à l'aide de la méthode ```.find()```. Complétez les fonctions ci-dessous, sans oublier de rattraper les exceptions s'il y a un problème quelconque avec votre review d'entrée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13935e2f-de5b-4b3d-b4a5-911ed6a033dc",
   "metadata": {},
   "source": [
    "Attention, les fonctions ci-dessous demandent parfois de récupérer autre chose que du texte, il faudra donc réfléchir à des alternatives à la méthode ```.text```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365bbf2-ebf5-48c3-8a80-6b8de4289801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_note_review(review):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        review: contenu HTML sous la forme d'une soupe BeautifulSoup (str)\n",
    "\n",
    "    Outputs:\n",
    "        note: Note de l'avis (entier entre 1 et 5)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edc31c-88fd-444c-9e6a-8eda7113483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_text = requests.get(url_company, headers=headers).text\n",
    "soup = BeautifulSoup(request_text, 'html.parser')\n",
    "review = soup.find('div', {'class':'styles_reviewCardInner__EwDq2'})\n",
    "note = review.find(\n",
    "    'div', {'class':'star-rating_starRating__4rrcf star-rating_medium__iN6Ty'}\n",
    ").find('img', alt=True)[\"alt\"]\n",
    "#print(review)\n",
    "print(note)\n",
    "print(type(note))\n",
    "#print(dir(note))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe104a-f940-4062-9642-2e87b647d083",
   "metadata": {},
   "source": [
    "Indices :\n",
    "- Pour récupérer un texte dans une balise _img_, un argument supplémentaire peut être passé à la fonction ```.find()```,  il s'agit de ```alt=True```\n",
    "- Penser à convertir le texte récupéré en une note sous la forme d'entier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e46f88-1547-40ac-bb3a-9dc8448a89f9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Dérouler pour révéler le corrigé</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def get_note_review(review):\n",
    "    \n",
    "    try:\n",
    "        texte_note = review.find(\n",
    "            'div', {'class':'star-rating_starRating__4rrcf star-rating_medium__iN6Ty'}\n",
    "        ).find('img', alt=True)['alt']\n",
    "        note = int(texte_note[5])  # texte_note = \"Noté x étoiles sur 5\"\n",
    "        return note\n",
    "\n",
    "    except:\n",
    "        return \"Not found\"\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79070f1-3743-43b6-9ae7-57b46e751ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_review(review):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        review: contenu HTML sous la forme d'une soupe BeautifulSoup (str)\n",
    "\n",
    "    Outputs:\n",
    "        date: Date et heure de l'avis (datetime)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab9ded-5f6d-4ff3-bb6a-20a280fde382",
   "metadata": {},
   "source": [
    "Indice : La fonction ```datetime.strptime()``` permet de convertir la chaîne de caractères récupérée en un format date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc153086-85f2-4b81-8a9a-aafe45e48ae5",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Dérouler pour révéler le corrigé</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def get_time_review(review):\n",
    "    \n",
    "    try:\n",
    "        str_date = review.find(\n",
    "            'div', {'class':'typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l styles_datesWrapper__RCEKH'}\n",
    "        ).find('time')['datetime']\n",
    "        date = datetime.strptime(str_date, \"%Y-%m-%dT%H:%M:%S.%fZ\")  # str_date = 'YYYY-MM-DDThh:mm:ss.000Z'\n",
    "        return date\n",
    "\n",
    "    except:\n",
    "        return \"Not found\"\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f337a-c380-419d-bcaf-8d2ee73df4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_review(review):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        review: contenu HTML sous la forme d'une soupe BeautifulSoup (str)\n",
    "\n",
    "    Outputs:\n",
    "        title: Titre de la review (str)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    return ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6a4b2-f2b7-4714-b7cd-5d40a1a20f22",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Dérouler pour révéler le corrigé</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def get_title_review(review):\n",
    "    \n",
    "    try:\n",
    "        title = review.find(\n",
    "            'h2', {'class':'typography_heading-s__f7029 typography_appearance-default__AAY17'}\n",
    "        ).text\n",
    "        return title\n",
    "\n",
    "    except:\n",
    "        return \"Not found\"\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb5f30-90df-4ca9-a041-8f06b35b08b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_review(review):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        review: contenu HTML sous la forme d'une soupe BeautifulSoup (str)\n",
    "\n",
    "    Outputs:\n",
    "        comment: Commentaire associé à la review (str)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    return ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92162bd3-ea50-49a1-b230-a92448b82fac",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Dérouler pour révéler le corrigé</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def get_comment_review(review):\n",
    "    \n",
    "    try:\n",
    "        text = review.find(\n",
    "            'p', {'class':'typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn'}\n",
    "        ).text # Les balises <br> sont supprimées, on va donc forcer des espaces après les points\n",
    "        text = re.sub(' +', ' ', text.replace('.', '. '))\n",
    "        return text\n",
    "\n",
    "    except:\n",
    "        return \"Not found\"\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b140978-d047-46f1-909b-7cb980fc3159",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Scraper toutes les reviews d'une entreprise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627fd57-aaab-49e6-9659-25245e97e491",
   "metadata": {},
   "source": [
    "Nous sommes désormais capables de récupérer toutes les informations contenues dans une review donnée.\n",
    "Reste maintenant à itérer sur l'ensemble des reviews d'une page, puis sur l'ensemble des pages (limité à 5 pages pour ne pas surcharger le site cible).\n",
    "\n",
    "Complétez la fonction ci-dessous pour récupérer l'ensemble des informations extraites des reviews en un dataframe aisément manipulable.\n",
    "\n",
    "Attention : bien penser à ajouter des pauses entre chaque requête d'au moins 3 secondes à l'aide de la commande ```time.sleep(3)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ece84e-ee8c-4585-9cb0-1b5eb61328cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da98101-089b-4ca2-98ea-72df48c5c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_one_company(url_company, limit_pages=True):\n",
    "\n",
    "    nb_pages = get_nb_pages_reviews(url_company)\n",
    "    if limit_pages and nb_pages > 5:\n",
    "        nb_pages = 5\n",
    "    notes, times, titles, comments = [], [], [], []\n",
    "\n",
    "    for page in tqdm(range(1, nb_pages+1)):\n",
    "\n",
    "        pass  # TODO\n",
    "\n",
    "    df_reviews = pd.DataFrame({\n",
    "        'note': notes,\n",
    "        'date': times,\n",
    "        'title': titles,\n",
    "        'comment': comments\n",
    "    })\n",
    "\n",
    "    return df_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318d4ef-2b49-45de-ab34-77f89179c965",
   "metadata": {},
   "source": [
    "Indices :\n",
    "- Quel va être l'url à requêter pour chaque page ?\n",
    "- Souvenez-vous de la fonction ```.find_all()```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef015f9-4c82-47dc-8db6-c56602f7cc92",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Dérouler pour révéler le corrigé</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def scraping_one_company(url_company, limit_pages=True):\n",
    "\n",
    "    nb_pages = get_nb_pages_reviews(url_company)\n",
    "    if limit_pages and nb_pages > 5:\n",
    "        nb_pages = 5\n",
    "    notes, times, titles, comments = [], [], [], []\n",
    "\n",
    "    for page in tqdm(range(1, nb_pages+1)):\n",
    "\n",
    "        time.sleep(3)\n",
    "        url_page = url_company + '?page=' + str(page)\n",
    "        \n",
    "        try:\n",
    "            request_text = requests.get(url_page, headers=headers).text\n",
    "            soup = BeautifulSoup(request_text, 'html.parser')\n",
    "            reviews = soup.find_all(\n",
    "                'div', {'class':'styles_cardWrapper__LcCPA styles_show__HUXRb styles_reviewCard__9HxJJ'}\n",
    "            )\n",
    "        except:\n",
    "            reviews = []\n",
    "\n",
    "        notes.extend(list(map(get_note_review, reviews)))\n",
    "        times.extend(list(map(get_time_review, reviews)))\n",
    "        titles.extend(list(map(get_title_review, reviews)))\n",
    "        comments.extend(list(map(get_comment_review, reviews)))\n",
    "\n",
    "    df_reviews = pd.DataFrame({\n",
    "        'note': notes,\n",
    "        'date': times,\n",
    "        'title': titles,\n",
    "        'comment': comments\n",
    "    })\n",
    "\n",
    "    return df_reviews\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d680a1-2fab-4a6c-8ab8-79a2fbe6ce27",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Collecte et vérification des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6370dc-bc7f-4442-aedc-3770d16b9bc9",
   "metadata": {},
   "source": [
    "Maintenant nos fonctions codées, il ne reste plus qu'à mettre en pratique et récupérer nos avis !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a08f1-6ed1-4a59-9503-708f0c963856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = scraping_one_company(url_company, limit_pages=True)\n",
    "\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bae4da5-030b-431f-8cbf-0e51e1f2d3a4",
   "metadata": {},
   "source": [
    "Lorsque l'on scrape des données sur Internet, la qualité attendue n'est pas toujours au rendez-vous.\n",
    "Il convient donc de bien vérifier ce qui est obtenu en sortie pour ne pas avoir de mauvaises surprises.\n",
    "Prenez un moment pour analyser vos données de sortie :\n",
    "\n",
    "- Le format des colonnes est-il bon ? Les textes ont-ils tous bien une apparence lisible ?\n",
    "- Y a-t-il des NaNs dans certaines colonnes ? Autrement dit des informations possiblement manquantes pour certaines reviews sur le site ?\n",
    "- Avez-vous bien le nombre de lignes attendu en sortie ?\n",
    "\n",
    "Pourquoi ne pas faire quelques visualisations avec matplotlib pour regarder à quoi ressemblent les distributions de nos variables ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a58d5c-2961-435e-b65f-17dc0421fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e138ea-cf9e-4f5e-88e5-73e266244de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amusez-vous ici avec df_reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cbe95b-6f1b-4925-b0bf-d6fe0ff0c0f9",
   "metadata": {},
   "source": [
    "La partie scraping est à présent terminée.\n",
    "La table que vous trouverez pour la seconde partie du sujet a été obtenue par scraping en itérant ce que vous avez recodé sur une liste donnée d'entreprises.\n",
    "\n",
    "Si vous souhaitez en savoir plus sur le monde du web scraping ou si vous voulez réitérer l'expérience sur d'autres sites, n'hésitez pas à consulter les ressources mentionnées au début du sujet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9f5f2-5692-42af-86ca-75646213fa03",
   "metadata": {},
   "source": [
    "## Partie 2 : Analyse textuelle et NLP\n",
    "\n",
    "Premier point de contact : Olivier Meslin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4663c8-3f2e-4cc5-a4e4-cb18b8671ace",
   "metadata": {},
   "source": [
    "La seconde partie du sujet consiste à analyser les données collectées. En particulier, étant donné que l'on dispose des commentaires laissés par les clients ainsi que leur note, il est intéressant de se demander dans quelle mesure il est possible de **prédire la note laissée par un client selon l'évaluation associée**. Pour ce faire, on va devoir coupler les méthodes de traitement du langage naturel (NLP) et celles d'apprentissage statistique (machine learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f3bbd-b258-4388-90ca-019b1c8765a1",
   "metadata": {},
   "source": [
    "### Import des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e078b-a397-46cb-aaa0-ee9f9e33ff70",
   "metadata": {},
   "source": [
    "Commençons par importer les données sur lesquelles on va baser notre analyse. Il s'agit des mêmes données que celles obtenues en première partie du sujet, itérées sur plusieurs entreprises. Nous nous sommes assurés du fait qu'il ne s'agissait là que de données en français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17446ea-0666-40fa-978a-27beb2b347b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c808e-4fe1-47e2-8f34-3a696cbd3f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"https://minio.lab.sspcloud.fr/projet-funathon/2024/sujet4/diffusion/reviews_planes.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dcee2b-9e08-4b99-8947-d715a0963c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae4ef0d-2fd6-4e0c-9e20-9687681f5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f833cd9a-7df6-413f-a796-c4bf4996c53c",
   "metadata": {},
   "source": [
    "Dans ce sujet, nous nous limiterons aux reviews contenant bien un commentaire non vide, ce qui n'est pas toujours le cas ici. Vérifier le type de chaque variable, le nombre de valeurs manquantes (ou ici de chaînes de caractères vides) est de façon générale une bonne pratique lorsque l'on est face à une nouvelle base de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2812599-7d72-42c6-996e-c91408a6166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre de reviews : {len(df)}\")\n",
    "df[\"comment\"] = df[\"comment\"].str.strip()  # Retirer les espaces en début ou fin de commentaire, pour aussi retirer les commentaires de type \"  \"\n",
    "df = df[df[\"comment\"] != \"\"]\n",
    "print(f\"Nombre de reviews avec un commentaire : {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccce9a1-9753-491e-84ff-c008784e3f92",
   "metadata": {},
   "source": [
    "### Analyse descriptive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae5391-7ce5-42f3-b807-040530f2d8cc",
   "metadata": {},
   "source": [
    "Avant de se lancer dans l'analyse textuelle à proprement parler, il est toujours préférable de commencer par une analyse descriptive très générale des données. Cela permet souvent de se faire une première idée de ce qu'elles contiennent et de réaliser des premières hypothèses sur le lien entre commentaire et note, que la modélisation viendra ensuite confirmer ou infirmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e31b9e-f3db-487e-bdb8-37c529338197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ce4fa-6448-4a0b-b4e6-be527c548249",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Les entreprises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b43511d-d8e8-4748-9cf0-b24d62f7f045",
   "metadata": {},
   "source": [
    "Représentons tout d'abord la part des différentes entreprises dans notre base. On note une surreprésentation claire de *www.fr.lastminute.com*, et dans une moindre mesure de *budgetair.fr*. Dans une analyse rigoureuse de machine learning, il faut tenir compte de ce type de déséquilibre qui peut biaiser les prédictions du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e774c-a0d9-453f-bb24-3619fc1a056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pourcentage d'avis en fonction de l'entreprise concernée\n",
    "df[\"company\"].value_counts(normalize=True).plot.pie(autopct=\"%.1f%%\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5983ed8b-65ea-4466-954c-48eef114b2f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Les notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a78de7-ceba-41f3-a28d-858fbc6aff77",
   "metadata": {},
   "source": [
    "Intéressons-nous ensuite à la répartition des notes. Il apparaît que les avis sont fortement polarisés: si plus de 60% des utilisateurs sont satisfaits de leur achat (4-5), plus d'un quart d'entre eux sont très insatisfaits (1). En revanche, les notes intermédiaires (2-3) sont sous-représentées et regroupent moins de 10% des avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966e7f3-bb8c-4ec0-a4b6-55822b323435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répartition des notes\n",
    "df[\"note\"].value_counts(normalize=True, sort=False).reset_index().sort_values(\"note\")[\"proportion\"].plot.pie(labels = range(1, 6), autopct=\"%.1f%%\", startangle = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cfcefe-c6ee-4c0a-9f06-5d1316d566e3",
   "metadata": {},
   "source": [
    "Pour réaliser le scraping, on a dû choisir les entreprises cibles et lire quelques avis afin de comprendre la structure HTML des pages. Ces premiers avis semblaient donner l'impression que *budgetair.fr* était particulièrement plébiscité, là où au contraire *www.fr.lastminute.com* semblait très largement associé à des expériences négatives. Est-ce effectivement le cas dans les données complètes ? Pour le vérifier, on analyse la proportion des notes pour ces deux entreprises particulières. Le constat est sans appel et confirme nos premières conjectures !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef7d2b0-bb31-4ae1-be1e-29b2a83b796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répartition des notes pour des entreprises données\n",
    "plt.figure(figsize=[12,6])\n",
    "colors = sns.color_palette('coolwarm', n_colors=5)\n",
    "\n",
    "plt.subplot(121)\n",
    "data = df[df['company'] == 'www.fr.lastminute.com'][\"note\"].value_counts(normalize=True, sort=False).sort_index()\n",
    "labels = data.keys()\n",
    "plt.pie(x=data, autopct=\"%.1f%%\", labels=labels, colors = colors, pctdistance=0.5)\n",
    "plt.title(\"www.fr.lastminute.com\", fontsize=14);\n",
    "\n",
    "plt.subplot(122)\n",
    "data = df[df['company'] == 'budgetair.fr'][\"note\"].value_counts(normalize=True, sort=False).sort_index()\n",
    "labels = data.keys()\n",
    "plt.pie(x=data, autopct=\"%.1f%%\", labels=labels, colors = colors, pctdistance=0.5)\n",
    "plt.title(\"budgetair.fr\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53997f4e-01a1-45ed-b1d8-d25833c86883",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Les dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42426d6c-494f-4577-bfc3-577dcc085dd1",
   "metadata": {},
   "source": [
    "A vous de jouer à présent !\n",
    "Représentez le nombre d'avis scrapés sur Trustpilot __par mois__ en fonction du temps.\n",
    "Y a-t-il des observations que vous pouvez faire sur cette évolution ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd25ed8-840f-4424-aa3b-419d61d6cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5d70d-1b96-461c-b707-92906a957b2e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Dérouler pour révéler le corrigé</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "nb_avis_par_mois = df.groupby(pd.Grouper(key='date', freq='M')).size()\n",
    "\n",
    "# Création du graphique\n",
    "plt.figure(figsize=(12, 6))\n",
    "nb_avis_par_mois.plot(kind='line', marker='o')\n",
    "plt.xlabel('Mois')\n",
    "plt.ylabel('Nombre d\\'avis laissés')\n",
    "plt.title('Nombre d\\'avis laissés sur les entreprises cibles par mois sur Trustpilot')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "On peut observer que le nombre d'avis collectés croît nettement de 2016 à 2019, puis connaît une évolution très heurtée en 2020-2021 (probablement en raison de la pandémie de Covid), avant de se stabiliser à un niveau proche de son niveau de fin 2019-début 2020..\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1d798c-f292-415b-a3fc-9be152259144",
   "metadata": {},
   "source": [
    "Pour aller plus loin, on peut différencier le graphique précédent par entreprise pour savoir comment se comporte chacune d'elles sur le site.\n",
    "Représentez donc sur un même graphe le nombre d'avis scrapés sur Trustpilot par __an__ en fonction du temps, différencié par entreprise.\n",
    "Quelles conclusions pouvez-vous tirer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659ca31-1428-4cbe-9f6b-2ff7f64b83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5cee4c-85d3-4584-ad54-613f1f45c665",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Dérouler pour révéler le corrigé</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "nb_avis_par_an_par_entreprise = df.groupby([pd.Grouper(key='date', freq='YE'), 'company']).size().unstack()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for company in nb_avis_par_an_par_entreprise.columns:\n",
    "    nb_avis_par_an_par_entreprise[company].plot(kind='line', marker='o', label=company)\n",
    "\n",
    "plt.xlabel('Année')\n",
    "plt.ylabel('Nombre d\\'avis scrapés')\n",
    "plt.title('Nombre d\\'avis laissés par entreprise sur les entreprises cibles par mois sur Trustpilot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "On peut observer que Lastminute.com fait l'objet d'un très grand nombre d'avis en 2019, puis que ce nombre décroît fortement en 2020 pour se stabiliser au tiers du niveau de 2019. Les avis sur BudgetAir ont également connu une évolution heurtée. En revanche, le nombre d'avis par compagnie est stable en 2022-2023.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc69878-e486-47f8-b211-7bc563ab42e8",
   "metadata": {},
   "source": [
    "Si vous souhaitez aller encore plus loin, vous pouvez représenter la note moyenne donnée par entreprise et par mois en fonction du temps, pour voir si la tendance des notes donnée à une entreprise est à la hausse ou à la baisse, et ainsi voir si le service s'améliore ou non par rapport à la concurrence et au passé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e53ba4-e4e9-4545-80cf-6f0a3cae45e7",
   "metadata": {},
   "source": [
    "#### Les commentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c549b-37a7-49f3-be01-763d5330d695",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Longueurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b7fc5-e5d6-46be-92b8-ea5951c36bb3",
   "metadata": {},
   "source": [
    "Intéressons-nous à présent au contenu des commentaires. Pour réaliser des analyses pertinentes à partir de données textuelles, il est indispensable d'utiliser les outils spécialisés du NLP, que ce soit pour nettoyer les données ou bien pour les exploiter. C'est ce que nous ferons dans les prochaines sections du tutoriel. Pour le moment, on peut d'ores et déjà s'intéresser à quelques caractéristiques très générales des textes, comme le nombre de caractères, la présence de majuscules ou de points d'exclamation, qui peuvent indiquer une certaine polarité des commentaires.\n",
    "\n",
    "On représente d'abord le lien entre le nombre de caractères d'un commentaires et la note associée, sous forme de boîtes à moustache par note. La première représentation semble indiquer que la longueur des commentaires tend à décroître lorsque la note augmente, mais les nombreuses valeurs extrêmes, symbolisées par les points, déforment l'échelle du graphique et peuvent empêcher de bien voir les écarts entre les boîtes. Ceci dit, il est souvent intéressant d'analyser de plus près ces valeurs extrêmes, car elles peuvent vous apprendre des choses sur les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138ae5b-9d75-478b-807a-127937048edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lien entre la longueur d'un commentaire et la note\n",
    "df['comment_length'] = df['comment'].str.len()\n",
    "\n",
    "sns.boxplot(x=\"note\", y=\"comment_length\", data=df)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c5ee27-4eb8-4215-a1ae-11a06613ce27",
   "metadata": {},
   "source": [
    "Pour limiter cet effet, on s'intéresse aux commentaires de moins de 1000 caractères, soit la large majorité d'entre eux. On perçoit ainsi mieux le gradient existant entre les boîtes. Il semble à ce stade que les consommateurs insatisfaits laissent en moyenne des commentaires plus longs, et inversement, et cet effet semble assez continu avec le niveau de notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1dfd62-5553-43d4-bba5-79085b30566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"note\", y=\"comment_length\", data=df[df['comment_length'] < 1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219337b-4f9f-4053-a5d4-4b2dc1e5aea5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Expressions régulières"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813c0fb-2e9b-40b8-a2d7-7cbe3cda6185",
   "metadata": {},
   "source": [
    "On peut ensuite vouloir par exemple détecter des signes d'énervement, comme la présence de nombreuses majuscules ou de points d'exclamation. On pourrait faire ces calculs en Python, mais cela serait fastidieux à coder. Pour détecter la présence de caractères ou séries de caractères dans un texte (des *patterns*), on utilise une syntaxe particulière, celle des expressions régulières (*regex*).\n",
    "\n",
    "La maîtrise des *regex* est un long voyage semé de frustration. Heureusement, on va se contenter dans ce tutoriel de motifs de base, très simples à comprendre. De plus, le site [regex101](https://regex101.com/) est très utile pour tester interactivement ses idées de regex : on y insère sa regex dans la case *REGULAR EXPRESSION* et le texte duquel on veut extraire de l'information dans la case *TEST STRING*, et l'on voit directement ce qui sera détecté (*matché*) dans le texte en question par la regex. Le site fournit par ailleurs, en bas à droite de la page, un pense-bête des principales syntaxes utilisées pour construire des regex. \n",
    "\n",
    "Pour utiliser des regex en `Python`, on utilise le package `re` (pas besoin de l'installer, il fait partie des librairies standard, mais il faudra tout de même l'importer). Ensuite, il y a à chaque fois deux étapes :\n",
    "- on compile la regex que l'on souhaite utiliser via la fonction `re.compile`. Cela crée un \"objet\" qui modélise la regex souhaitée.\n",
    "- on utilise les méthodes (fonctions associées à des objets) de l'objet pour extraire de l'information. En l'occurrence, on va utiliser la méthode `findall` pour compter le nombre d'occurrences du *pattern* dans le texte.\n",
    "\n",
    "Pour chaque cas, on définit une fonction qui prend en entrée un texte et retourne en sortie respectivement la proportion de majuscules et de points d'exclamation. Puis on utilise la méthode `apply` des séries `Pandas` pour appliquer ces fonctions à chacun de nos commentaires, en créant des nouvelles colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880b845-b391-4a88-a42d-7f03f6d30757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6e1ec-4807-49c6-aac2-16f967fa8279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pour extraire des infos du texte\n",
    "\n",
    "def compte_majuscules(commentaire):\n",
    "    r = re.compile(r\"[A-Z]\")  # Regex qui match toute majuscule\n",
    "    capslock = r.findall(commentaire)  # Compte le nombre de majuscules dans le texte\n",
    "    return len(capslock) / len(commentaire)  # Normalisation par la longueur du texte\n",
    "\n",
    "\n",
    "def compte_points_exclamation(commentaire):\n",
    "    r = re.compile(r\"\\!\")  # Regex qui match tout point d'exclamation\n",
    "    exclamation = r.findall(commentaire)  # Compte le nombre de points d'exclamation dans le texte\n",
    "    return len(exclamation) / len(commentaire)  # Normalisation par la longueur du texte\n",
    "\n",
    "\n",
    "df['part_majuscules'] = df['comment'].apply(compte_majuscules)\n",
    "df['part_points_exclamation'] = df['comment'].apply(compte_points_exclamation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d37a2-5cc0-4597-b69e-5c0fcb1f90b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"comment\", \"part_majuscules\", \"part_points_exclamation\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181afd7d-dd5d-4eae-90e4-27262ea723b6",
   "metadata": {},
   "source": [
    "A présent, on va s'intéresser au lien entre la présence ou non de ces *patterns* dans les commentaires et la note donnée. Pour cela, on va utiliser des *violin plots*, du fait de leur forme en violon. L'idée est proche de la boîte à moustache, dans la mesure où l'on retrouve celle-ci et les statistiques qu'elle fournit (médiane, quartiles..) mais avec en plus un concept de **densité**, qui permet de voir rapidement la concentration des observations à certains niveaux de la distribution. Là encore, on va couper les valeurs extrêmes afin d'obtenir une échelle plus représentative de l'ensemble de la distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62de37a-9d57-4607-a715-9c706e514a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lien entre le nombre de majuscules et la note\n",
    "data = df[df[\"part_majuscules\"] < 0.1]\n",
    "sns.violinplot(x=\"note\", y=\"part_majuscules\", data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43681b09-9ea4-4666-b0d4-c9700d65117a",
   "metadata": {},
   "source": [
    "Les groupes des notes 4 et 5 se détachent légèrement du reste, avec une part plus importante de majuscules en moyenne. C'est également le cas du groupe 1, mais de façon moins mette. Ce constat, s'il est difficile à établir statistiquement, n'en reste pas moins logique à première vue : les consommateurs très satisfaits et inversement très mécontents ont tendance à utiliser davantage de majuscules que les autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4535873-5505-4ec9-b147-21456c578266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lien entre le nombre de points d'exclamation et la note\n",
    "data = df[df[\"part_points_exclamation\"] < 0.005]\n",
    "sns.violinplot(x=\"note\", y=\"part_points_exclamation\", data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f42c98-7e11-4dd5-a914-71c1dfb639be",
   "metadata": {},
   "source": [
    "Pour ce qui est des points d'exclamation, le constat est un peu moins net. On voit néanmoins que les distributions des groupes des notes 1 et 2 sont plus étirées en longueur, ce qui indique une proportion un peu plus forte de points d'exclamation en moyenne, tandis que les distributions des groupes 4 et 5 sont nettement plus concentrées vers 0. Ce constat est tout aussi logique que le précédent: les consommateurs très insatisfaits ont tendance à utiliser davantage de points d'exclamation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251ebbc4-c101-48ea-9d85-05a5d1405b55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### A vous de jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4782e41-c6de-4aa9-9821-6fd4b62df90a",
   "metadata": {},
   "source": [
    "Répliquez l'analyse précédente en utilisant d'autres expressions régulières que vous pensez pertinentes.\n",
    "Êtes-vous capables d'obtenir une meilleure corrélation avec la note que ce qui est présenté ci-dessus ?\n",
    "\n",
    "Quelques exemples que vous pouvez utiliser :\n",
    "- Le nombre de points d'interrogation dans le commentaire\n",
    "- Le nombre de points d'exclamation successifs\n",
    "- La recherche de smileys, comme \"^^\" ou bien \":(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007275d6-977c-439a-b370-f4131e54adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92994ed4-ee35-4351-ac82-769df3fb5d0e",
   "metadata": {},
   "source": [
    "Globalement, on voit que ces statistiques très généralistes sur les textes ont leur utilité, mais ne sauraient suffire à les distinguer. Il va donc falloir aller plus loin et **s'intéresser aux mots utilisés par les clients et en dégager une polarité**. Mais comment extraire l'information présente dans un texte sous une forme \"compréhensible\" par l'ordinateur, et donc utilisable pour prédire la note associée à un commentaire selon son contenu ?\n",
    "\n",
    "La réponse à cette question n'est pas triviale, et de nombreuses méthodes concurrentes existent. Dans ce tutoriel, on va s'intéresser aux **méthodes classique de l'analyse textuelle, qui consistent à résumer l'information présente dans un texte par la fréquence des mots qu'il contient**. L'idée est intuitive : si l'on observe que les commentaires ayant une note faible contiennent souvent le mot \"manquant\" et inversement les commentaires ayant une note élevée le mot \"parfait\", il est clair que l'on pourra utiliser cette information pour prédire la note associée à un commentaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6b8a59-30da-4aeb-a214-ba9cd1755a5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Nettoyage et prétraitements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935865ae-cebd-48ef-a13f-1ca10631ca0b",
   "metadata": {},
   "source": [
    "Pour qu'une analyse fréquentielle ait du sens, il est nécessaire au préalable de bien nettoyer et pré-traiter les données textuelles. L'idée générale de cette partie est qu'il est nécessaire de *normaliser* au maximum les textes afin que les fréquences calculées soient les plus pertinentes possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5949c4-e564-4a78-be3b-1c6be96e39d4",
   "metadata": {},
   "source": [
    "Les premières normalisations à effectuer sont assez naturelles :\n",
    "- On va transformer tous les textes en minuscules. On utilise pour ce faire les fonctions de `Pandas`.\n",
    "- On va retirer tous les accents. On utilise pour ce faire la fonction `unidecode` du package du même nom, qui va transformer tous les caractères en leur représentation [ASCII](https://fr.wikipedia.org/wiki/American_Standard_Code_for_Information_Interchange) la plus proche, une nomenclature très basique qui contient seulement les caractères essentiels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9322d-8a76-4cad-849a-ff60316f7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f3a704-f269-4308-8f23-1aab466d6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copier le commentaire et commencer à le normaliser\n",
    "df[\"comment_norm\"] = df[\"comment\"]\n",
    "df[\"comment_norm\"] = df[\"comment_norm\"].str.lower()\n",
    "df[\"comment_norm\"] = df[\"comment_norm\"].apply(unidecode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d84d45-c9d4-4d00-b10e-c5d33d9a84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"comment\", \"comment_norm\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f1154-b249-4faa-8042-4f2dd59901be",
   "metadata": {},
   "source": [
    "On va ensuite vouloir procéder à des normalisations un peu plus fines : supprimer la ponctuation, supprimer les chiffres, supprimer les mots trop fréquents, qui n'ont pas de sens particulier mais peuvent biaiser l'analyse, etc. On pourrait essayer de faire tout ça à la main en `Python`, mais non seulement cela serait fastidieux, cela serait aussi assez compliqué en pratique : comment gère-t-on le cas d'un apostrophe ? Comment gère-t-on le cas des nombres décimaux séparés par une virgule ou un point selon les régions ? Comment définit-on un mot trop fréquent ?\n",
    "\n",
    "Heureusement, de nombreuses personnes avant nous se sont posé ses questions, de sorte qu'il existe des outils très efficaces et bien pensés pour se faire. La librairie standard pour effectuer ce genre de traitements en `Python` s'appelle `nltk`. Dans ce tutoriel, on va plutôt présenter la librairie `spaCy`, plus moderne et souvent beaucoup plus performante lorsqu'il s'agit de traiter un large corpus de textes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f01bf-41eb-4788-a9bc-fe11ca6f2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae2010f-07a7-4682-adf8-d7238e08469f",
   "metadata": {},
   "source": [
    "La grande force de `spaCy` est de proposer des modèles pré-entrainés pour de nombreuses langues. Très concrètement, il s'agit de modèles d'apprentissage statistique, en général entraînés via des méthodes de *deep-learning*, que l'on a entraîné à différentes tâches (ex : reconnaître les entités nommées, reconnaître la fonction grammaticale d'un mot, etc.) sur d'énormes volumes de texte. Par conséquent, ils sont capables de comprendre finement la structure du langage, et vont nous être utiles pour normaliser au maximum nos textes.\n",
    "\n",
    "On commence donc par charger le modèle français. On prend le moyen (`md` pour *medium*), qui donne généralement des résultats satisfaisants tout en restant de taille limitée, mais notons qu'on pourrait avoir de meilleurs résultats en utilisant le modèle `lg` (*large*). Dans le cadre de ce tutoriel, pour les utilisateurs du SSP Cloud, le modèle a déjà été pré-téléchargé. En pratique, dans un environnement vierge, par exemple si vous ne passez pas par le Datalab, il faudrait le télécharger pour pouvoir l'utiliser avec la commande suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b0d11-3da5-470b-8ffd-a340529ddf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db23d4-0ab7-4ad9-912a-96d54092a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f254ab-fe5e-4760-a91c-1f855db4936f",
   "metadata": {},
   "source": [
    "L'objet `Python` que l'on récupère s'applique directement aux textes comme une fonction, et permet de faire une multitude d'actions sur ces textes. Regardons ces différentes possibilités avec un exemple simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a7328a-a82b-4a2d-9677-b2fa5e80be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_norm = df[\"comment_norm\"][0]\n",
    "doc = nlp(comment_norm)\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68d5608-84a7-463f-8f09-7122040bbe8f",
   "metadata": {},
   "source": [
    "Jusque là rien de bien révolutionnaire : on voit que l'on peut imprimer l'objet `doc` renvoyé par le modèle lorsqu'on lui fournit un texte en entrée. Encore heureux..\n",
    "\n",
    "Mais cet objet `doc` contient de fait tout un ensemble de caractérisations sur le texte qui vont nous être bien utiles. En particulier, il s'est chargé d'une tâche essentielle : la ***tokenisation***. Il s'agit du procédé qui consiste à convertir un texte en une série de *tokens*, i.e. de petites unités ayant une valeur sémantique. Il peut s'agir des mots, des caractères, de groupes de caractères, etc. Dans notre cas, on va vouloir faire une analyse fréquentielle à partir des termes utilisés dans les commentaires, il est donc raisonnable de séparer le texte en mots.\n",
    "\n",
    "L'objet `nlp` découpe par défaut le texte initial en *tokens*. Il suffit d'itérer sur cet objet pour passer en revue les différents *tokens*. Par ailleurs, `spaCy` détecte automatiquement un ensemble d'informations bien utiles sur les *tokens* :\n",
    "- la fonction grammaticale (attribut `.tag_`)\n",
    "- est-ce qu'il s'agit d'un *stop-word*, i.e. d'un mot très fréquent et donc qui n'apporte généralement pas d'information pertinente (attribut `.is_stop`)\n",
    "- est-ce qu'il s'agit d'un élément de ponctuation (attribut `.is_punct`)\n",
    "- est-ce qu'il ne contient que des caractères alphabétiques (attribut `.is_alpha`)\n",
    "\n",
    "Et tout un tas d'autres choses. N'hésitez pas à explorer la très complète [documentation](https://spacy.io/usage/linguistic-features) pour une vue détaillée des possibilités de `spaCy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d333fbce-1315-470e-882b-c63dfa351ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regarder les 15 premiers tokens du premier avis\n",
    "for token in doc[:15]:\n",
    "    print(token.text, token.tag_, token.is_stop, token.is_punct, token.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649878b9-0fd6-4dcf-b617-f1c68c0a1a64",
   "metadata": {},
   "source": [
    "En particulier, un dernier attribut va nous intéresser dans notre entreprise de normalisation des textes : la ***lemmatisation***. Cela consiste à remplacer chaque mot par sa forme neutre canonique. En effet, si l'on calcule les fréquences d'apparition des différents mots pour caractériser les textes, on voudrait par exemple que les mots \"manquant\", \"manquants\" ou encore \"manquante\" soient comptés comme un seul et même mot, dans la mesure où ils portent la même valeur sémantique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5cbc72-5be8-4edb-b702-b7b96938b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:15]:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080aa273-3cc5-4f0d-8da0-ccd9f0929127",
   "metadata": {},
   "source": [
    "Armé de tous ces attributs bien utiles que nous offre à peu de frais `spaCy`, on va normaliser nos textes de la manière suivante :\n",
    "- suppression des *stop-words*\n",
    "- suppression de la ponctuation\n",
    "- suppression des mots non-alphabétiques (chiffres, etc.)\n",
    "- suppression des *tokens* de trois caractères ou moins\n",
    "\n",
    "Illustrons comment cela se fait pour un texte donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd3cc8-f374-4634-9962-6d803b38965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_lemma_list = []\n",
    "\n",
    "for token in doc:\n",
    "    # On rajoute un token à la liste s'il remplit les conditions suivantes:\n",
    "    # Le token fait au minimum 3 caractères, est un mot et n'est ni un stopword ni un signe de ponctuation\n",
    "    if not (token.is_stop or token.is_punct) and token.is_alpha and len(token) >= 3:\n",
    "        no_stop_lemma_list.append(token.lemma_)\n",
    "\n",
    "# On concatène les éléments de la liste, séparés par un espace\n",
    "no_stop_lemma = \" \".join(no_stop_lemma_list)\n",
    "\n",
    "print(doc)\n",
    "print()\n",
    "print(no_stop_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136755e-ad64-4e3c-9869-3d076c22c7f6",
   "metadata": {},
   "source": [
    "Pas mal ! Maintenant on voudrait bien appliquer cela à tous les textes de notre corpus. En réalité, même si cela ne s'est pas vu sur un seul document, ce processus est assez coûteux en temps. Heureusement, `spaCy` nous permet de largement l'accélérer :\n",
    "- on va paralléliser les calculs, de sorte que 5 textes vont être normalisés en parallèle\n",
    "- on va supprimer les fonctions de `spaCy` dont on ne va pas se servir en pratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4d843e-9b52-456b-9fe3-601b2566de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = nlp.pipe(df[\"comment_norm\"], n_process=5,\n",
    "                disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff34ac-f9a9-4938-b73b-cd4c53a64e93",
   "metadata": {},
   "source": [
    "Une fois l'objet `pipe` défini, on peut itérer sur les commentaires par une simple boucle. On va construire notre processus étage par étage :\n",
    "- une fonction `preprocess_token` qui prend un *token* en entrée, retourne sa version lemmatisée si jamais il respecte les conditions énoncées précédemment, et une chaîne de caractères vide sinon\n",
    "- une fonction `preprocess_text` qui prend en entrée un texte au format objet `spaCy` (i.e. un élément de l'objet `pipe`), qui applique à chaque *token* la fonction `preprocess_token` en supprimant au passage les *tokens* vides, et renvoie le texte normalisé sous la forme d'une seule chaîne de caractères\n",
    "\n",
    "Il nous suffit ensuite d'appliquer la fonction `preprocess_text` à chaque élément de l'objet `pipe`, i.e. chaque commentaire, pour obtenir la liste des textes normalisés. Cette opération ne devrait pas prendre plus d'une minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028efa9f-7675-421b-83d3-58bf375bee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_token(token):\n",
    "    if not (token.is_stop or token.is_punct) and token.is_alpha and len(token) >= 3:\n",
    "        return token.lemma_\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text(text_nlp):\n",
    "    text_pretraite_list = [preprocess_token(token) for token in text_nlp if token]\n",
    "    text_pretraite = \" \".join(text_pretraite_list)\n",
    "    return text_pretraite\n",
    "\n",
    "commentaires_pretraites = [preprocess_text(commentaire) for commentaire in pipe]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845d9dc-99c3-4b2c-9679-802f970394e4",
   "metadata": {},
   "source": [
    "Finalement, on peut rajouter à notre dataset la version normalisée des commentaires, et vérifier que les résultats font effectivement sens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745f2e5-4494-4826-980b-df7035c1ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"comment_pretraite\"] = commentaires_pretraites\n",
    "df[[\"title\", \"comment\", \"comment_norm\", \"comment_pretraite\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62cdcdb-0b1b-46a7-b22e-0cd8c48c53b4",
   "metadata": {},
   "source": [
    "### Analyse textuelle des commentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb43b95-b446-4819-9620-40b90c601acd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Nuages de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad188848-8b44-4e76-a81f-5e8b6d0e4d85",
   "metadata": {},
   "source": [
    "Petit exercice en marge de la suite : maintenant les commentaires nettoyés et les stopwords retirés, il est possible de générer une visualisation intéressante supplémentaire : les __nuages de mots__. Il s'agit d'une représentation des mots les plus représentés dans notre base de commentaires, dont les tailles grandissent avec leur volume dans le corpus.\n",
    "\n",
    "Exécutez les cellules ci-dessous pour comprendre comment cela fonctionne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701f5721-412b-4310-adad-9c5b38727a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29389fee-0972-4f90-9015-76b43af3cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_cleaned_comments = ' '.join(df['comment_pretraite'])  # Concaténer tout le corpus en une chaîne de caractères\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(corpus_cleaned_comments)  # Créer le nuage de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283f3d1-3693-499e-8ec0-84526ffb0edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le nuage de mots\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad6632-f976-4e4c-aeb4-781424dca0ec",
   "metadata": {},
   "source": [
    "__Exercice__ : A vous de jouer maintenant, en vous inspirant du code ci-dessus, créez un nuage de mots par entreprise pour toutes les entreprises présentes dans la base de données. Voyez-vous des différences significatives entre les nuages de mots des unes et des autres ? Comparez notamment le nuage de mots de Lastminute et de BudgetAir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558977e0-e813-4313-be5e-493cfd40b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd912f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Dérouler pour révéler le corrigé</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# Lister les compagnies par nombre décroissant d'avis\n",
    "liste_companies = df.groupby(['company'])[\"company\"].size().reset_index(name='count').sort_values(['count'], ascending=False)[\"company\"]\n",
    "liste_companies\n",
    "\n",
    "# Faire les nuages de mots pour Lastminute et Budgetair\n",
    "for company in liste_companies[0:2]:\n",
    "    corpus_cleaned_comments_company = ' '.join(df.query(\"company == @company\")['comment_pretraite'])  # Concaténer le corpus d'une compagnie en une chaîne de caractères\n",
    "    wordcloud_company = WordCloud(width=800, height=400, background_color='white').generate(corpus_cleaned_comments_company)  # Créer le nuage de mots\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud_company, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(company, fontsize=14);\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a41bb3-4498-4086-aa6c-cabc47f33b70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a3ef2-fbe3-4430-bcd2-da1158f5572a",
   "metadata": {},
   "source": [
    "A présent, nos textes sont relativement normalisés, ce qui va nous permettre d'effectuer une analyse fréquentielle digne de ce nom. La prochaine étape va consister à transformer notre information textuelle en une information numérique, compréhensible par des machines et donc exploitable dans une analyse statistique. Cette étape s'appelle la **vectorisation** : on va transformer une liste de *tokens* en un vecteur numérique. Il existe de nombreuses méthodes permettant de vectoriser un texte, avec plus ou moins de complexité.\n",
    "\n",
    "Commençons par détailler la plus intuitive : la **vectorisation par comptage**. Cette méthode consiste simplement à représenter chaque texte par un vecteur qui pour chaque mot du vocabulaire indique la fréquence du mot dans le texte. L'exemple ci-dessous illustre ce procédé pour un texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c5d083-8770-4f4c-abd0-7fc0453816b8",
   "metadata": {},
   "source": [
    "![](img/countvectorizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87d99c-c0bc-4f6b-807c-d10282903171",
   "metadata": {},
   "source": [
    "Bien sûr, l'idée est d'appliquer cette opération non pas à un seul texte mais à tous les textes de notre corpus. Il faut pour cela que les vecteurs de comptage soient comparables. On va donc procéder en plusieurs étapes :\n",
    "- récupérer le vocabulaire complet du corpus, i.e. tous les tokens uniques présents dans les textes du corpus;\n",
    "- les classer par ordre alphabétique;\n",
    "- représenter chaque texte par un vecteur de fréquence au sein du vocabulaire complet. Ainsi, la matrice qui va représenter le corpus complet va être dite creuse (*sparse*), car chaque texte ne contient que quelques mots du vocabulaire complet.\n",
    "\n",
    "Là encore, on pourrait faire tout ça en `Python`, mais ça serait pénible et sûrement inefficient. Heureusement, le package `scikit-learn`, référence pour le *machine learning* en `Python`, nous permet de faire ça très rapidement en quelques lignes de code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceab6c91-63f3-4bac-9fd5-1686ff04f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec4e8b-5c71-4202-bb4e-e0fd2ad4a376",
   "metadata": {},
   "source": [
    "En pratique :\n",
    "- on instancie un objet de type `CountVectorizer`;\n",
    "- on l'\"entraîne\" (*fit*) sur notre corpus, ce qui a pour effet de récupérer le vocabulaire complet et de le classer par ordre alphabétique;\n",
    "- on vectorise nos textes (*transform*) pour construire la matrice représentant le corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73053a3f-fb51-42d0-a8a6-f876b9c8f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"comment_pretraite\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(texts)\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36472443-6e23-4f96-a081-574156695bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_vectorized = vectorizer.transform(texts)\n",
    "texts_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64848ef2-305d-4d7f-803c-2fdfddfdb894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice : à quoi correspondent les dimensions de la matrice ?\n",
    "\n",
    "print(f\"Nombre de documents dans le corpus : {len(df)}\")\n",
    "print(f\"Taille du vocabulaire : {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2386170b-9d2d-43cd-b92c-578de8275c34",
   "metadata": {},
   "source": [
    "Comme notre matrice est largement creuse, `scikit-learn` la stocke au format `sparse matrix`, qui permet de prendre beaucoup moins de place en mémoire que si l'on avait stocké en dur toutes les cellules qui contiennent \"0\". Dans notre cas, comme le corpus est de taille raisonnable, on peut la récupérer sous forme d'un `array NumPy` sans trop de problème, ce qui facilitera les manipulations par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4918fb-3542-4b54-8ae3-2261d4a31807",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_vectorized = texts_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47709c-26a5-43a1-8ba2-bcdcb1517558",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b80271-3f7e-44e2-937f-e52fb56c2307",
   "metadata": {},
   "source": [
    "Le fait d'avoir vectorisé notre corpus de la sorte va maintenant nous permettre de réaliser une analyse statistique agrégée avec là encore peu de code. Commençons par afficher les mots les plus fréquents de notre corpus. Pour cela, à vous de jouer :\n",
    "- Sommez la matrice par colonne afin d'obtenir la fréquence de chaque mot dans le corpus complet\n",
    "- Associez dans une liste chaque mot à sa fréquence totale\n",
    "- Classez les mots par fréquence totale décroissante\n",
    "- Insérez les résultats dans un `DataFrame Pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cdfb27-9e93-4907-b62d-269e1e9137e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "comptages_voc = pd.DataFrame(\n",
    "    {'mot': [\"exemple\"],\n",
    "     'frequence': [1]}\n",
    ")  # TODO\n",
    "\n",
    "comptages_voc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e7425-230b-44b5-8231-00d9b05dbf78",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Dérouler pour révéler le corrigé</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "comptages = texts_vectorized.sum(axis=0)\n",
    "comptages_voc = list(zip(vocab, comptages))\n",
    "comptages_voc = sorted(comptages_voc, key=lambda x: x[1], reverse=True)\n",
    "comptages_voc = pd.DataFrame(comptages_voc, columns=['mot', 'frequence'])\n",
    "comptages_voc.head()\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Si une ou plusieurs de ces étapes ne sont pas claires, n'hésitez pas à aller lire la documentation des fonctions correspondantes pour mieux comprendre leur fonctionnement.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b71143f-d728-46b1-b986-704184f1ec1c",
   "metadata": {},
   "source": [
    "On peut alors représenter très simplement les fréquences des n premiers mots du corpus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5adadd3-ba23-4b42-b04a-4d4b73d15c58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=\"frequence\", y=\"mot\", data=comptages_voc.head(10))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6455dff-0041-4eae-a4b4-bd69e63b8cb8",
   "metadata": {},
   "source": [
    "Si votre fonction est bien codée (ou que vous reprenez le corrigé), alors à première vue, les résultats semblent cohérents. Notons que c'est bien parce que l'on a décidé d'enlever les *stop-words* qu'on retrouve les mots qu'on attendait dans ce graphique. Sinon, il est très probable que l'on aurait eu en premier \"de\", \"à\", \"le\", etc. et que l'analyse n'aurait donc produit aucun résultat intéressant.\n",
    "\n",
    "Notons également que la vectorisation telle que nous l'avons effectué ne tient aucun compte de l'ordre des mots dans la phrase, de la grammaire, etc. On dit qu'il s'agit d'une approche ***bag-of-words*** : le texte est vu comme un sac de mots, dont l'ordre n'importe pas, seules leurs fréquences dans le texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a1a341-caf6-472d-a9ea-9b0f1f32ba45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### n-grammes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d83d7-52a3-421c-9cba-48f2c0e81cd4",
   "metadata": {},
   "source": [
    "Tenir compte de l'ordre des mots dans la vectorisation implique tout de suite beaucoup plus de complexité dans l'analyse. Une manière relativement simple de tenir - un peu - compte de cet ordre est de s'intéresser aussi aux ***n-grammes***, i.e. aux suites de mots consécutifs. On peut s'intéresser aux bigrammes, i.e. aux suites de deux mots consécutif, aux trigrammes, etc. Mais attention : plus l'on inclut de n-grammes, plus le vocabulaire grandit rapidement, et plus l'analyse sera coûteuse en temps et en mémoire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5423f154-cd1c-42c6-be7d-d426f60c6e1e",
   "metadata": {},
   "source": [
    "**Exercice** : en s'inspirant du code précédent et de la [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) du `CountVectorizer` de `scikit-learn`, créer une fonction `plot_freq_n_grammes` qui :\n",
    "- applique le `CountVectorizer` aux textes pour un range de n-grammes donné (paramètre de la fonction);\n",
    "- construit le `DataFrame` des comptages (cf. ci-dessus);\n",
    "- renvoie les n premières lignes du `DataFrame` (paramètre de la fonction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b0b4a-b6bc-4c6d-a446-f91945433047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_freq_n_grammes(commentaires, ngram_range=(1,1), n_results=10):\n",
    "    \n",
    "    comptages_voc = pd.DataFrame(columns=['mot', 'frequence'])  # TODO  \n",
    "    \n",
    "    return comptages_voc.head(n_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a08402c-3538-4d81-9ba8-54898600355d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Dérouler pour révéler le corrigé</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def plot_freq_n_grammes(commentaires, ngram_range=(1,1), n_results=10):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    vectorizer.fit(commentaires)\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    comptages = np.array(vectorizer.transform(commentaires).sum(0))[0]\n",
    "    comptages_voc = list(zip(vocab, comptages))\n",
    "    comptages_voc = sorted(comptages_voc, key=lambda x: x[1], reverse=True)\n",
    "    comptages_voc = pd.DataFrame(comptages_voc, columns=['mot', 'frequence'])  \n",
    "    \n",
    "    return comptages_voc.head(n_results)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Si une ou plusieurs de ces étapes ne sont pas claires, n'hésitez pas à aller lire la documentation des fonctions correspondantes pour mieux comprendre leur fonctionnement.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2e9d2-10e0-4646-8f43-3e64180c33a3",
   "metadata": {},
   "source": [
    "Vérifier tout d'abord que la fonction fonctionne dans le cas des mots simples, i.e. des unigrammes. Le code suivant devrait produire le même graphique que celui ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df78084-dc71-4d8e-a605-f408a8d765e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comptages_1_1 = plot_freq_n_grammes(commentaires=texts, \n",
    "                                    ngram_range=(1,1), \n",
    "                                    n_results=10)\n",
    "\n",
    "sns.barplot(x=\"frequence\", y=\"mot\", data=comptages_1_1)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f176b-b626-444f-9f44-0959b2b3524e",
   "metadata": {},
   "source": [
    "Maintenant que l'on dispose d'une fonction, il est très facile de faire varier l'étendue des *n-grammes* et de produire les mêmes graphiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6bc3b-735a-478a-87ef-a6479f6fe5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comptages_2_2 = plot_freq_n_grammes(commentaires=texts, \n",
    "                                    ngram_range=(2,2), \n",
    "                                    n_results=10)\n",
    "\n",
    "sns.barplot(x=\"frequence\", y=\"mot\", data=comptages_2_2)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8045a04c-ad3b-4117-a5cd-fd5a1328b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "comptages_3_3 = plot_freq_n_grammes(commentaires=texts, \n",
    "                                    ngram_range=(3,3), \n",
    "                                    n_results=10)\n",
    "\n",
    "sns.barplot(x=\"frequence\", y=\"mot\", data=comptages_3_3)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c93e9-b92e-4111-9453-b71a7d708380",
   "metadata": {},
   "source": [
    "__Exercice__ : Réaliser les mêmes visualisations en restreignant les données en entrée aux commentaires avec une note de 1, puis aux commentaires avec une note de 5. Dnas quelle mesure le top 10 des mots les plus fréquent change-t-il? Est-ce pertinent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f87672f-ee34-4e1a-b69f-994889f87686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f079f5a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Dérouler pour révéler le corrigé</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "texts = df[df[\"note\"] == 1][\"comment_pretraite\"]\n",
    "\n",
    "comptages_3_3 = plot_freq_n_grammes(commentaires=texts, \n",
    "                                    ngram_range=(3,3), \n",
    "                                    n_results=10)\n",
    "\n",
    "sns.barplot(x=\"frequence\", y=\"mot\", data=comptages_3_3)\n",
    "plt.show();\n",
    "\n",
    "texts = df[df[\"note\"] == 1][\"comment_pretraite\"]\n",
    "\n",
    "comptages_3_3 = plot_freq_n_grammes(commentaires=texts, \n",
    "                                    ngram_range=(3,3), \n",
    "                                    n_results=10)\n",
    "\n",
    "sns.barplot(x=\"frequence\", y=\"mot\", data=comptages_3_3)\n",
    "plt.show();\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "dudu\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961b192-1741-408a-b3c9-136abe13f009",
   "metadata": {},
   "source": [
    "### Apprentissage supervisé : prédiction de la note à partir du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad95de8-ba9f-42fd-afdc-2deaf73f0c6e",
   "metadata": {},
   "source": [
    "Les analyses descriptives de la partie précédente nous ont bien montré que l'analyse fréquentielle semblait pertinente, dans la mesure où certains mots ou groupes de mots se retrouvaient prédominants respectivement dans les groupes de notes très basses ou très hautes. Dans cette partie, nous allons aller encore un cran plus loin, et nous demander dans quelle mesure il est possible de prédire la note associée à un commentaire, seulement à partir des mots contenus dans ce dernier. \n",
    "\n",
    "Etant donné qu'il s'agit d'une tache de prédiction, on va utiliser les outils du machine learning. En pratique, on va entraîner un modèle, i.e. fournir de nombreux exemples (note, commentaire) issus de notre corpus au modèle, de sorte à ce qu'il apprenne la relation entre les mots présents et la note donnée, et qu'il puisse ensuite prédire sur de nouveaux exemples (sans note).\n",
    "\n",
    "En machine learning, on commence toujours par séparer les donnéees d'entraînement (comme expliqué ci-dessus) et les données de test, c'est à dire des données qui n'auront pas directement servi à l'entraînement et que l'on pourra donc utiliser pour évaluer rigoureusement la performance de notre modèle. Il est d'usage de garder 20% des données comme jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aae62a-04d2-4213-adc9-a65b9b11214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a01eb8-1d67-4ecc-9d02-ec38ad5c8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4acc90-c47d-4300-a7a2-2cde5b0d76d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train[\"note\"]\n",
    "y_test = df_test[\"note\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d9e37-47cd-4fe8-b9db-93cf45bb28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3090542-df4a-443a-b98d-cc78e57341da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Vectorisation par comptage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d631f92-59b3-49cd-81fd-de2e508fb2b0",
   "metadata": {},
   "source": [
    "Un modèle de machine learning prend en entrée des données au format numérique. Là encore, nous allons nous baser sur l'analyse fréquentielle, i.e. vectoriser nos commentaires de manière à pouvoir les passer au modèle. Une première approche est celle que nous avons vue dans l'analyse descriptive ci-dessus, dite de vectorisation par comptage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a9cdb-fbb0-4172-88fd-a85e23be4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea95cdf-1da5-4a8a-a444-90487e0b5084",
   "metadata": {},
   "source": [
    "L'analyse descriptive a montré que les bigrammes et les trigrammes avaient leur pertinence. On avait donc appliquer un `CountVectorizer` avec un range de n-grammes allant de 1 à 3 inclus. On l'entraîne (*fit*) sur les données d'entraînement. Sur les données de test, on ne fait que transformer, i.e. vectoriser sous forme d'une matrice ```(n_documents, longueur_vocab)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675c880-0626-4ce9-abbc-3966f6f14f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "X_train = vectorizer.fit_transform(df_train[\"comment_pretraite\"])\n",
    "X_test = vectorizer.transform(df_test[\"comment_pretraite\"])\n",
    "\n",
    "voc = vectorizer.vocabulary_\n",
    "print('Taille du vocabulaire : ', len(voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5de6c9-5e80-408b-9567-4cf09d31c625",
   "metadata": {},
   "source": [
    "A partir de cette matrice, on va pouvoir entraîner un modèle à prédire la note associée à un texte. Par simplicité, on va assimiler la tâche de prédiction à une tâche de régression, i.e. que le modèle va devoir prédire une note continue et être le plus proche possible de la note entre 1 et 5 initiale. Pour ce qui est du modèle, on va utiliser un modèle de machine à vecteur de support (SVM) linéaire, car ceux-ci sont connus pour être efficaces sur de telles tâches de NLP. Le fonctionnement du SVM n'est pas nécessaire à la compréhension de ce que l'on essaie de faire : retenons simplement que le modèle va apprendre les liens entre fréquences des mots dans les commentaires et la note associée au commentaire, pour pouvoir ensuite prédire une note à partir d'un commentaire seul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7c4c2-2901-4cee-9253-194d8ac43a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4179505-0710-40df-8dfe-95a2c2d1a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = LinearSVR(max_iter=10000)\n",
    "\n",
    "svr.fit(X_train, y_train)  # Entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe790d3-1d00-4f9c-ab3f-dbaa48b9bafd",
   "metadata": {},
   "source": [
    "Une fois le modèle entraîné, on peut l'utiliser pour prédire une note à partir d'un commentaire. On prédit donc des notes pour les données de test, ce qui va nous permettre de comparer la note prédite et la note réelle, et donc d'évaluer la qualité des prédictions. On utilise pour cela une métrique standard pour les tâches de régression : l'erreur moyenne quadratique. Là encore, la définition exacte de cette métrique n'est pas nécessaire à la compréhension de ce tutoriel, elle va simplement de nous permettre de comparer les performances de différentes modélisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a96b7bb-c500-4625-936e-f9d84f9fc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e74b82-f437-4eeb-83ca-22c4030cc130",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svr.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81de4d-9f82-4d1d-90c4-5a7a8609da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd65a38-007b-4e48-b9aa-92da245bb5b1",
   "metadata": {},
   "source": [
    "__Exercice__ : Quelle est l'erreur moyenne absolue (MAE) associée à notre modèle ? Comment l'interpréter ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4940b-98da-4c8f-a851-99528fb30fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62396c4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Dérouler pour révéler le corrigé</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_pred, y_test)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "La valeur de l'erreur moyenne absolue mesure la distance moyenne entreee la prédiction et la vraie valeur. En l'occurrence, la note prédite est en moyenne séparée de 0.9 point de la vraie valeur.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ed093-58d0-4704-b701-86ff9699e2a0",
   "metadata": {},
   "source": [
    "L'analyse quantitative donne une métrique qui résume les performances globales. Mais on peut également se demander pour quels groupes de notes le modèle obtient les meilleurs performances, ou bien au contraire quels groupes il ne parvient pas à distinguer. Pour cela, une analyse qualitative est nécessaire. On va simplement mettre en correspondant les notes prédites et les notes réelles sous forme de *violin plots* par groupe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b1403-05ab-4347-89fa-0deb065eafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse qualitative\n",
    "data = pd.DataFrame({'note': y_test, 'note_predite': y_pred})\n",
    "\n",
    "sns.violinplot(x=\"note\", y=\"note_predite\", data=data)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad58175d-97d0-4357-8d06-a2599a02c077",
   "metadata": {},
   "source": [
    "Comme le SVM effectue une prédiction continue de la note, celle-ci n'est pas bornée entre 1 et 5 en toute rigueur. En pratique, on voit qu'il parvient à concentrer les notes prédites dans cette intervalle la plupart du temps, mais qu'il effectue parfois des prédictions totalement en dehors qui déforment l'échelle du graphique. Là encore, on se restreint donc aux valeurs les plus fréquentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24adf6dd-0ae3-4714-b2e8-80cf9e378b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=\"note\", y=\"note_predite\", data=data[(data[\"note_predite\"] > 0) & (data[\"note_predite\"] <= 5)])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae58f44-8f48-4190-bbc4-386f30c35fa3",
   "metadata": {},
   "source": [
    "A première vue, ce n'est pas si mal ! On observe bien un gradient assez continu au fil des classes de notes, ce qui indique que le modèle parvient à distinguer en général les classes de commentaires. On voit cependant que cette performance n'est pas homogène selon les classes : les classes de note 4 et 5 sont très bien prédites même si le modèle a du mal à les distinguer. En revanche pour les classes 1 à 3, les distributions sont très plates et peu concentrées autour de la vraie valeur, même si on observe un net gradient sur la médiane de la distribution (petit tiret blanc).\n",
    "\n",
    "Naturellement, on se demande comment on pourrait faire mieux. En NLP, c'est toujours une question compliquée car les facteurs qui contribuent à la performance sont multiples. On pourrait ainsi :\n",
    "- essayer d'améliorer la qualité du pré-traitement du texte (nettoyage, normalisation...);\n",
    "- essayer d'améliorer la qualité de la vectorisation en utilisant d'autres méthodes plus avancées que le simple comptage;\n",
    "- essayer d'utilliser d'autres modèles que le SVM pour la prédiction;\n",
    "- essayer de *fine-tuner* le modèle de prédiction.\n",
    "\n",
    "Dans ce tutoriel, nous allons nous concentrer sur la vectorisation, car c'est ce qui présente le plus d'intérêt du point de vue du traitement du langage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa7cb43-26a5-4ad4-8573-2242425279f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Approche TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc8428-fe56-4972-bf78-45e7c8eeff32",
   "metadata": {},
   "source": [
    "La vectorisation par simples comptages produit déjà des résultats intéressants, mais reste assez rudimentaire : on représente un texte par le vecteur des fréquences des termes qui le composent. Mais le fait qu'un terme apparaisse fréquemment dans un texte ne suffit pas à lui attribuer de l'importance : si le terme est par ailleurs très fréquent dans le corpus de textes complet, le fait qu'il soit fréquent dans un texte donné n'est pas très intéressant. Par exemple, le fait qu'un commentaire contienne les termes \"avion\" et \"réservation\" n'aide pas beaucoup à prédire la note laissée par le client, dans la mesure où ces termes sont très fréquents dans les commentaires. Pour pallier cette limite, on va modifier la manière de vectoriser en repondérant les fréquences de sorte à donner plus de poids à un terme qui apparaît dans un texte donné, mais qui est par ailleurs rare dans le corpus. Ce sont en effet de tels termes qui ont a priori le plus de chance d'être discriminants pour le modèle d'apprentissage.\n",
    "\n",
    "Cette repondération a un nom : __TF-IDF__. Vous en avez sans doute déjà entendu parler et pour cause : ce modèle a longtemps été utilisé par Google pour affiner les résultats des recherches ! Formellement, le modèle est le suivant :\n",
    "\n",
    "![](./img/tfidf.png)\n",
    "\n",
    "Source : [http://notanotherdatafact.com](http://notanotherdatafact.com/index.php/2020/09/02/tf-idf-un-jour-tf-idf-toujours/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82f782-d006-42f9-abb4-d607b110d666",
   "metadata": {},
   "source": [
    "Comment implémenter cette nouvelle méthode de vectorisation ? Grâce à l'incroyable qualité de la standardisation des objects du package `scikit-learn`, la procédure revient exactement au même ! Il va suffire de changer le *vectorizer* et tout le reste va suivre immédiatement. Pas mal, non ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36bf1fc-86d4-4cbf-ae4a-90ecb2e6a0b8",
   "metadata": {},
   "source": [
    "__Exercice__ : Reproduire l'analyse précédente en utilisant un vectorizer TF-IDF au lieu du CountVectorizer. Vous pouvez vous aider de la documentation de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d939444-da8d-42a8-baaa-99a5fdf251c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db147774-ffa5-41e8-83b8-53f5056fe487",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font size=2 color=\"red\"><b>Dérouler pour révéler le corrigé</b></font> </summary>\n",
    "<br>\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer2 = TfidfVectorizer(ngram_range=(1,3))\n",
    "```\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Pour le reste, les cellules de code ne changent pas. Voici le reste du code à exécuter: \n",
    "\n",
    "\n",
    "```python\n",
    "X_train = vectorizer2.fit_transform(df_train[\"comment_pretraite\"])\n",
    "X_test = vectorizer2.transform(df_test[\"comment_pretraite\"])\n",
    "\n",
    "voc = vectorizer2.vocabulary_\n",
    "print('Taille du vocabulaire : ', len(voc))\n",
    "\n",
    "svr2 = LinearSVR(max_iter=10000)\n",
    "svr2.fit(X_train, y_train)  # Entraînement\n",
    "\n",
    "y_pred2 = svr2.predict(X_test)\n",
    "print(mean_squared_error(y_pred2, y_test))\n",
    "\n",
    "# Analyse qualitative\n",
    "data2 = pd.DataFrame({'note': y_test, 'note_predite': y_pred2})\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(x=\"note\", y=\"note_predite\", data=data2[(data2[\"note_predite\"] > 0) & (data2[\"note_predite\"] <= 5)])\n",
    "plt.show();\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b8d4-24ce-4a31-ac83-1b4d7da97b76",
   "metadata": {},
   "source": [
    "Si vous ne vous êtes pas trompés, le verdict est clair : la vectorisation TF-IDF améliore considérablement les performances, que ce soit sur le plan quantitatif ou qualitatif. Les classes 4 et 5 sont toujours correctement prédites, mais surtout les classes 1, 2 et 3 sont mieux prédites par le modèle; la médiane de la distribution prédite est notamment alignée sur la vraie valeur de chaque classe. Enfin, la distribution des notes prédites pour la classe 1 est beaucoup plus concentrées autour de la vraie valeur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21986dc9-442e-4269-aab4-32b828d59bdd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ea37c-0e5c-494b-b29d-0566aaf3765c",
   "metadata": {},
   "source": [
    "Quelles sont les conclusions que vous pouvez apporter sur les différences de performances entre les deux approches ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213dfbdf-7759-491c-b1e5-979b3ef84496",
   "metadata": {},
   "source": [
    "Avancé :\n",
    "- Essayez d'autres modèles de ```scikit-learn``` que le SVM (par exemple un RandomForestRegressor) et comparez les performances obtenues\n",
    "- Rajoutez des régresseurs à l'approche TF-IDF et voyez si les performances augmentent. Quelques exemples de régresseurs possibles :\n",
    "  - Longueur du commentaire\n",
    "  - Indicatrice de l'entreprise dont vient le commentaire\n",
    "  - Indicatrice de l'année du commentaire pour tenir compte de l'évolution des marques\n",
    "  - Proportion de points d'exclamation\n",
    "  - Proportion de majuscules\n",
    "  - ...\n",
    "- Répliquez les analyses précédentes sur la colonne \"Titre\" plutôt que celle des commentaires. Comment évolulent les résultats ?\n",
    "  - Quid de répliquer le travail sur une colonne concaténée \"Titre\" + \"Commentaire\" lorsque le titre n'est pas inclus dans le commentaire ? Les résultats sont-ils significativement améliorés ?\n",
    " \n",
    "Avec ces différents suppléments, quelles sont les performances maximales que vous parvenez à atteindre ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a3954-c7a7-4660-9b3f-e19dc6b7ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eabc21-c1f7-4d47-a315-89f6b6812b50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Apprentissage non-supervisé : détection de sujets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324b18a-b0cb-4b12-a39b-200c59bb658e",
   "metadata": {},
   "source": [
    "Les tâches de prédiction sur lesquelles nous nous sommes concentré correspondent à de l'apprentissage supervisé : chaque commentaire est associé à une note, que l'on peut aussi appeler _label_ et que l'on cherche à prédire.\n",
    "Une autre branche du machine learning existe et permet de travailler sur des données non labellisées, par exemple pour faire du clustering ou des nuages de mots.\n",
    "\n",
    "Ici, nous allons faire de l'apprentissage non supervisé pour faire de la détection des sujets principaux sous-latents à nos commentaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812450a-c82f-4fb2-948e-7987aa414e1f",
   "metadata": {},
   "source": [
    "Pour ce faire, nous allons utiliser une méthode appelée LDA (Allocation de Dirichlet Latente), qui est en fait à un modèle génératif probabiliste permettant d'extraire automatiquement les sujets sous-jacents d'un corpus de documents.\n",
    "L'idée principale derrière la LDA est que chaque document peut être considéré comme une combinaison de sujets, et chaque sujet est une distribution de mots.\n",
    "Via une approche probabiliste, la LDA permet ainsi de découvrir les structures et les thèmes latents présents dans un corpus de documents, ce qui est particulièrement utile pour l'analyse de textes, la catégorisation de documents et l'exploration de données non structurées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07cb99-7edf-4bcc-8bfe-c82f3b8e2d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis.lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e3a3bb-b2bf-4932-9201-ff667eee49a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5, max_df=0.9, ngram_range=(1,3))\n",
    "freq_matrix = vectorizer.fit_transform(df[\"comment_pretraite\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1905c43a-0487-441f-a3a0-c92aff84dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=20, random_state=0)\n",
    "lda.fit(freq_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038d724-fa47-4084-a5ea-8cea8c000c34",
   "metadata": {},
   "source": [
    "LDAvis est une bibliothèque de visualisation conçue spécifiquement pour explorer et interpréter les résultats d'une analyse de LDA.\n",
    "Cette bibliothèque facilite la compréhension des structures de sujets identifiées par LDA en fournissant des visualisations interactives et informatives.\n",
    "\n",
    "L'un des paramètres importants de LDAvis est le paramètre d'arbitrage fréquence/salience. Ce paramètre permet de contrôler l'affichage des mots associés à chaque sujet dans la visualisation.\n",
    "La fréquence d'un terme représente le nombre d'occurrences du terme dans le corpus, tandis que la salience mesure l'importance relative d'un terme pour un sujet donné.\n",
    "En ajustant ce paramètre, vous pouvez choisir de mettre davantage l'accent sur les termes les plus fréquents ou sur ceux qui sont plus saillants pour chaque sujet.\n",
    "L'arbitrage fréquence/salience peut être utile pour mieux comprendre les sujets dominants et les termes clés associés à ces sujets.\n",
    "Si vous souhaitez identifier les termes les plus courants et les plus représentatifs d'un sujet, vous pouvez privilégier la fréquence.\n",
    "A l'inverse, si vous souhaitez mettre en évidence les termes distinctifs et significatifs qui différencient un sujet des autres, vous pouvez privilégier la salience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b700e-8516-4aab-ab84-80de1c3c539e",
   "metadata": {},
   "source": [
    "Exécutez la cellule ci-dessous pour voir à quoi ressemble la visualisation offerte par ```pyLDAvis```. Le paramètre λ modifiable en haut à droite de la fenêtre interactive correspond à l'arbitrage fréquence/salience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ff875-5cc9-42ea-9014-c267211c12e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.lda_model.prepare(lda, freq_matrix, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa509a-c218-4cef-8aae-639ca7dc9793",
   "metadata": {},
   "source": [
    "__Exercices__ :\n",
    "\n",
    "- Faites varier le paramètre λ dans LDAvis. Quelle est la valeur optimale en termes d'interprétabilité que vous pouvez trouver ?\n",
    "- Faites varier le nombre de sujets (```n_components```) pour trouver les meilleurs clusters possibles\n",
    "- Faites varier le preprocessing (min_df, max_df, ngram_range) et observez l'influence que cela peut avoir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb88721-a7db-4be5-95b1-8865f3b6542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfb61c4-8abc-4e18-a35d-ed9196c56977",
   "metadata": {},
   "source": [
    "### Pour aller plus loin : MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905ea20-488f-4f73-be1f-93dd93f83c73",
   "metadata": {},
   "source": [
    "A travers ces exemples vous avez pu avoir une première expérience de Machine Learning à des fins de NLP.\n",
    "Dans le monde réel, une bonne pratique lorsque vous faites du ML peut être d'utiliser _MLflow_ pour ce type d'usage.\n",
    "Pour plus d'informations :\n",
    "- La formation [Une introduction au MLOps avec MLflow](https://inseefrlab.github.io/formation-mlops/slides/fr/index.html) développée par la DIIT et le SSP Lab ;\n",
    "- Le [chapitre dédié](https://ensae-reproductibilite.github.io/website/chapters/mlops.html#impl%C3%A9mentation-de-lapproche-mlops-avec-mlflow) dans le cours de Mise en Production de l'ENSAE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2b224-611e-4126-bcaa-16a2a102879f",
   "metadata": {},
   "source": [
    "## Félicitations, vous avez désormais atteint la fin du sujet 4 !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16183c-f20f-461c-8085-a077ab58ba58",
   "metadata": {},
   "source": [
    "Si vous souhaitez répliquer les mêmes analyses dans le contexte des services de commande de repas en ligne, vous pouvez aller regarder le [sujet 5](https://github.com/InseeFrLab/funathon2023_sujet5) du funathon 2023.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
